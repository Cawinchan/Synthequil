{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv1d\n",
    "import torchaudio\n",
    "import utils    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Define device for torch\n",
    "use_cuda = True\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemixingAudioDataset(Dataset):\n",
    "    \"\"\"Demixing Audio dataset\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the audio.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "\n",
    "        Returns: \n",
    "            \n",
    "            sample (dict): No transform \n",
    "                    key (str): 'context',\n",
    "                    value (torch.Tensor): mixture_waveform\n",
    "                    key (str): 'target',\n",
    "                    values (tuple): (bass_waveform, drums_waveform ... )\n",
    "\n",
    "            sample (dict): With transform\n",
    "                    key (str): 'context',\n",
    "                    value (list of torch.Tensor): mixture_stft_batch\n",
    "                    key (str): 'target',\n",
    "                    values (tuple of list of torch.Tensor): (bass_stft_batch, drums_stft_batch ... )\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        size = len(glob.glob(\"{}/*\".format(self.root_dir)))\n",
    "        return size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        mixture_waveform, _ = torchaudio.load(\"{}/{}\".format(glob.glob(\"{}/*\".format(self.root_dir))[idx],'mixture.wav'))\n",
    "        bass_waveform, _ = torchaudio.load(\"{}/{}\".format(glob.glob(\"{}/*\".format(self.root_dir))[idx],'bass.wav'))\n",
    "        drums_waveform, _ = torchaudio.load(\"{}/{}\".format(glob.glob(\"{}/*\".format(self.root_dir))[idx],'drums.wav'))\n",
    "        others_waveform, _ = torchaudio.load(\"{}/{}\".format(glob.glob(\"{}/*\".format(self.root_dir))[idx],'other.wav'))\n",
    "        vocals_waveform, _ = torchaudio.load(\"{}/{}\".format(glob.glob(\"{}/*\".format(self.root_dir))[idx],'vocals.wav'))\n",
    "\n",
    "        sample = {'context': mixture_waveform, \n",
    "                    'target': (bass_waveform,\n",
    "                                drums_waveform,\n",
    "                                others_waveform,\n",
    "                                vocals_waveform)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class SplitAudio(object):\n",
    "    \"\"\"Splits audio into chunks\n",
    "\n",
    "    Args:\n",
    "        output_size (int): Desired output size of audio frames \n",
    "\n",
    "    returns: \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        context, target = sample['context'], sample['target']\n",
    "        mixture_waveform = context\n",
    "        bass_waveform, drums_waveform, others_waveform, vocals_waveform = target\n",
    "        \n",
    "        mixture_waveform_batch = torch.split(mixture_waveform,self.output_size,dim=1)\n",
    "        bass_waveform_batch = torch.split(bass_waveform,self.output_size,dim=1)\n",
    "        drums_waveform_batch = torch.split(drums_waveform,self.output_size,dim=1)\n",
    "        others_waveform_batch = torch.split(others_waveform,self.output_size,dim=1)\n",
    "        vocals_waveform_batch = torch.split(vocals_waveform,self.output_size,dim=1)\n",
    "\n",
    "\n",
    "        return {'context': mixture_waveform_batch, \n",
    "                    'target': (bass_waveform_batch,\n",
    "                                drums_waveform_batch,\n",
    "                                others_waveform_batch, \n",
    "                                vocals_waveform_batch)}\n",
    "\n",
    "class STFTWaveform(object):\n",
    "    \"\"\"Converts waveform to stft \n",
    "\n",
    "    \"\"\"\n",
    "    # FIXME: Fix reshaping if needed\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        context, target = sample['context'], sample['target']\n",
    "        mixture_waveform = context\n",
    "        bass_waveform, drums_waveform, others_waveform, vocals_waveform = target\n",
    "        \n",
    "        mixture_stft_batch = []\n",
    "        for waveform in mixture_waveform:\n",
    "            stft = torch.stft(waveform,n_fft=4096,hop_length=1024,win_length=4096,return_complex=False)\n",
    "            stft = torch.reshape(stft,(2, 2, 2049, -1)).to(device)\n",
    "            mixture_stft_batch.append(stft)\n",
    "\n",
    "        bass_stft_batch = []\n",
    "        for waveform in bass_waveform:\n",
    "            stft = torch.stft(waveform,n_fft=4096,hop_length=1024,win_length=4096,return_complex=False)\n",
    "            stft = torch.reshape(stft,(2, 2, 2049, -1)).to(device)\n",
    "            bass_stft_batch.append(stft)\n",
    "\n",
    "        drums_stft_batch = []\n",
    "        for waveform in drums_waveform:\n",
    "            stft = torch.stft(waveform,n_fft=4096,hop_length=1024,win_length=4096,return_complex=False)\n",
    "            stft = torch.reshape(stft,(2, 2, 2049, -1)).to(device)\n",
    "            drums_stft_batch.append(stft)\n",
    "        \n",
    "        others_stft_batch = []\n",
    "        for waveform in others_waveform:\n",
    "            stft = torch.stft(waveform,n_fft=4096,hop_length=1024,win_length=4096,return_complex=False)\n",
    "            stft = torch.reshape(stft,(2, 2, 2049, -1)).to(device)\n",
    "            others_stft_batch.append(stft)\n",
    "\n",
    "        vocals_stft_batch = []\n",
    "        for waveform in vocals_waveform:\n",
    "            stft = torch.stft(waveform,n_fft=4096,hop_length=1024,win_length=4096,return_complex=False)\n",
    "            stft = torch.reshape(stft,(2, 2, 2049, -1)).to(device)\n",
    "            vocals_stft_batch.append(stft)\n",
    "\n",
    "\n",
    "        return {'context': mixture_stft_batch, \n",
    "                    'target': (bass_stft_batch,\n",
    "                                drums_stft_batch,\n",
    "                                others_stft_batch, \n",
    "                                vocals_stft_batch)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([SplitAudio(44100),\n",
    "                               STFTWaveform()])\n",
    "\n",
    "train_data = DemixingAudioDataset(\"raw_data/train\",transform=composed)\n",
    "test_data = DemixingAudioDataset(\"raw_data/test\",transform=composed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    # FIXME: Fix the model\n",
    "\n",
    "\n",
    "    def __init__(self, numChannels, classes, fixedFirstLayer = 48):\n",
    "        super(CNN, self).__init__() \n",
    "\n",
    "        # Encoder \n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.relu1 = nn.GELU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=fixedFirstLayer, out_channels=2*fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.relu2 = nn.GELU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=2*fixedFirstLayer, out_channels=4*fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.relu3 = nn.GELU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=4*fixedFirstLayer, out_channels=8*fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.relu4 = nn.GELU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        #Bottleneck layer\n",
    "        self.conv5 = nn.Conv1d(in_channels=8*fixedFirstLayer, out_channels=16*fixedFirstLayer,kernel_size=(8, 8),stride=(8,8))\n",
    "        self.relu5 = nn.GELU()\n",
    "\n",
    "        # Decoder \n",
    "        self.de_conv1 = nn.ConvTranspose2d(in_channels=16*fixedFirstLayer, out_channels=8*fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.de_relu1 = nn.GELU()\n",
    "        self.de_maxpool1 = nn.UpsamplingBilinear2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.de_conv2 = nn.Conv1d(in_channels=8*fixedFirstLayer, out_channels=4*fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.de_relu2 = nn.GELU()\n",
    "        self.de_maxpool2 = nn.MaxUnpool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.de_conv3 = nn.Conv1d(in_channels=4*fixedFirstLayer, out_channels=2*fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.de_relu3 = nn.GELU()\n",
    "        self.de_maxpool3 = nn.MaxUnpool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.de_conv4 = nn.Conv1d(in_channels=2*fixedFirstLayer, out_channels=fixedFirstLayer,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.de_relu4 = nn.GELU()\n",
    "        self.de_maxpool4 = nn.MaxUnpool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.de_conv4 = nn.Conv1d(in_channels=fixedFirstLayer, out_channels=classes,kernel_size=(8, 8),stride=(4,4))\n",
    "        self.de_relu4 = nn.GELU()\n",
    "        self.de_maxpool4 = nn.MaxUnpool2d(kernel_size=(2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "    \n",
    "        embeddings = self.embeddings(inputs)\n",
    "\n",
    "        out = self.linear(embeddings)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, numChannels, classes, fixedFirstLayer = 48,):\n",
    "        super(simple_CNN, self).__init__() \n",
    "\n",
    "        # Encoder \n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=numChannels, out_channels=classes, kernel_size=(8, 8),stride=(4,4))\n",
    "        self.gelu1 = nn.GELU()\n",
    "        # self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "    \n",
    "        embeddings = self.conv1(inputs)\n",
    "        out = self.gelu1(embeddings)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_CNN(\n",
       "  (conv1): ConvTranspose2d(2, 4, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (gelu1): GELU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model and pass to CUDA if available.\n",
    "model = simple_CNN(numChannels = 2, classes = 4)\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "learning_rate = 0.0001 # Number recommended by wave-u-net paper\n",
    "epochs = 100\n",
    "torch.manual_seed(28)\n",
    "loss_function = nn.MSELoss() # Loss recommended by wave-u-net paper\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) # optimiser recommended by wave-u-net paper\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2049, 44])\n",
      "torch.Size([2, 4, 8200, 180])\n",
      "torch.Size([2, 1, 8200, 180])\n",
      "tensor([[[-2.2287e+00,  0.0000e+00,  4.6125e+00,  ...,  0.0000e+00,\n",
      "          -2.0750e+01,  0.0000e+00],\n",
      "         [-2.8599e+01,  0.0000e+00, -2.9630e+01,  ...,  0.0000e+00,\n",
      "          -9.9305e+00,  0.0000e+00],\n",
      "         [-4.5172e+00, -1.1921e-06,  3.6602e+00,  ...,  4.0574e+00,\n",
      "          -2.1347e+01,  3.2380e+00],\n",
      "         ...,\n",
      "         [-2.2142e-03,  4.2550e-07, -6.8724e-03,  ..., -3.3275e-01,\n",
      "          -1.2846e-01, -3.9848e-01],\n",
      "         [-1.5256e-01,  9.9499e-02,  3.9991e-01,  ...,  3.7075e-01,\n",
      "          -5.8186e-02, -5.0394e-01],\n",
      "         [-1.5503e-02,  0.0000e+00, -6.5674e-02,  ..., -3.3011e-01,\n",
      "           6.8115e-02, -7.9959e-01]],\n",
      "\n",
      "        [[-1.1847e-01, -7.1112e-01, -2.6312e-01,  ...,  1.1993e-02,\n",
      "          -1.2402e-01, -3.6142e-01],\n",
      "         [ 3.4067e-02,  3.0629e-07, -9.0276e-02,  ..., -1.0181e-01,\n",
      "           3.7164e-01, -7.3254e-01],\n",
      "         [ 4.8378e-01, -1.8970e-02, -1.1900e-01,  ...,  3.2134e-01,\n",
      "          -1.9532e-01, -5.5748e-01],\n",
      "         ...,\n",
      "         [ 4.2894e-02, -9.6350e-03,  1.3534e-02,  ..., -1.9073e-06,\n",
      "          -1.1134e-01,  9.1553e-05],\n",
      "         [-8.1177e-03,  0.0000e+00, -4.0405e-02,  ...,  0.0000e+00,\n",
      "           2.9245e-01,  0.0000e+00],\n",
      "         [ 3.6621e-02,  0.0000e+00,  2.2705e-02,  ...,  0.0000e+00,\n",
      "          -1.1600e-01,  0.0000e+00]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\money\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([2, 2049, 44])) that is different to the input size (torch.Size([2, 1, 8200, 180])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (180) must match the size of tensor b (44) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-57faaaf9e731>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-57faaaf9e731>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, model, epochs, loss_func, optimizer)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;31m# How do we know which index is which instrument?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_stft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_lst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_stft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;31m# Backward pass and optim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\money\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\money\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\money\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\money\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (180) must match the size of tensor b (44) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "def train(data, model, epochs, loss_func, optimizer):\n",
    "    \"\"\"\n",
    "    This is a trainer function to train our CNN model.\n",
    "    \"\"\"\n",
    "    # TODO: Use early stopper and tensorboard \n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for sample in data:\n",
    "            for keys, values in sample.items():\n",
    "                batch_stft = values[0]\n",
    "                target_stft = values[1]\n",
    "                total_loss = 0\n",
    "                # Forward pass\n",
    "                model.zero_grad()\n",
    "\n",
    "                # FIXME: Fix shape of output\n",
    "                output = model(batch_stft)\n",
    "                \n",
    "                print(output.shape)\n",
    "                output_lst = torch.split(output,1,dim=1)\n",
    "                # FIXME: Fix loss calculator\n",
    "\n",
    "                # How do we know which index is which instrument? \n",
    "                for i in range(len(target_stft)):\n",
    "                    loss = loss_func(output_lst[i], target_stft[i])\n",
    "\n",
    "                # Backward pass and optim\n",
    "                total_loss += loss.data.item()\n",
    "                # print(loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Loss update\n",
    "    \n",
    "    # Display\n",
    "    if epoch % 10 == 0:\n",
    "        # FIXME: Create accuracy checker data\n",
    "        accuracy = check_accuracy(model, data, word2index, index2word)\n",
    "        print(\"Accuracy after epoch {} is {}\".format(epoch, accuracy))\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(total_loss)\n",
    "    return losses, accuracies, model\n",
    "\n",
    "losses, accuracies, model = train(train_data, model, epochs, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c1b642627569bcc4b862c0530c2d477fffe7311302631907cc769f08d8277df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
